import requests
import os
import re
import io
from PyPDF2 import PdfReader

# --- USER CONFIGURATION ---
# Set the year you want to search for and save files from.
TARGET_YEAR = "2025" # <--- Change this to "2023", "2025", etc., to target a different year!

# Replace with your actual API credentials and URLs
client_id = "#Use your Client id" 
client_secret = "#Use your client_secret"
token_url = "#Use your token_url"
instance = "#Use your.service-now.com"

request_items_url = f"https://{instance}/api/xxxx/request_api/requestitems"      # Custom API to fetch request items
attachments_url = f"https://{instance}/api/now/table/sys_attachment"              # Standard ServiceNow API for attachments
download_url_template = f"https://{instance}/api/now/attachment/{{sys_id}}/file"  # Endpoint to download attachment by sys_id
assignment_group_sys_id = "Use your assignment_group_sys_id"

# Folder where PDFs will be saved (e.g., '2025_PDF_documents' or '2024_PDF_documents')
save_folder_base = r"C:\folder_location" #File Save Location
save_folder = os.path.join(save_folder_base, TARGET_YEAR)

# --- END USER CONFIGURATION ---

# Ensure the save folder for the target year exists
os.makedirs(save_folder, exist_ok=True)


# Query parameters for filtering request items
# IMPORTANT: Replaced query 'sys_created_on' with the actual field name "opened_at", ServiceNow used for creation date.
# Common alternatives: 'opened_at', 'u_date_field', etc.
# Also, ensure 'active=true' is valid if you use it.
#For a specific group of SERVICENOW tickets and time range

query_params = {
    "query": f"assignment_group={assignment_group_sys_id}" 
             f"^opened_atBETWEENjavascript:gs.dateGenerate('{TARGET_YEAR}-01-01','00:00:00')@javascript:gs.dateGenerate('{TARGET_YEAR}-12-31','23:59:59')", 
    #"limit": "8000",  # Max number of records to retrieve
    "fields": "sys_id, number,assignment_group, opened_at" # Include relevant fields, remove 'active' if not needed or not exposed by custom API
}

# ------------------- GET ACCESS TOKEN USING CLIENT CREDENTIALS -------------------

print("Authenticating with OAuth...")

auth_data = {
    "grant_type": "client_credentials",
    "client_id": client_id,
    "client_secret": client_secret,
    "scope": "api://your-scope-id/.default"  # Replace with actual resource App ID URI
}

token_response = requests.post(token_url, data=auth_data)
if token_response.status_code != 200:
    print("OAuth authentication failed:", token_response.text)
    exit()

access_token = token_response.json().get("access_token")
headers = {
    "Authorization": f"Bearer {access_token}",
    "Accept": "application/json"
}

# === API Request ===

print("Fetching request items...")
items_response = requests.get(request_items_url, headers=headers, params=query_params)
if items_response.status_code != 200:
    print("âŒ Failed to get request items:", items_response.text)
    exit()

items = items_response.json().get("result", [])
if not items:
    print("No request items found based on the filters.")
    exit()

# === Counters ===

saved_count = 0
non_target_year_skipped_count = 0

def download_pdf(file_resp, file_path, final_filename):
    if os.path.exists(file_path):
        print(f"â™»ï¸ Replacing existing file: {final_filename}")
    try:
        with open(file_path, "wb") as f:
            f.write(file_resp.content)
        print(f"âœ… Saved {final_filename}")
    except PermissionError:
        print(f"ðŸš« Cannot write to {file_path} â€” file may be open or locked.")

def is_duplicate(file_path, file_resp):
    return os.path.exists(file_path) and os.path.getsize(file_path) == len(file_resp.content)

# === Process each request item ===

for item in items:
    ritm_number = item.get("number", "")
    req_sys_id = item["sys_id"]
    print(f"\nðŸ“¦ Processing item: {ritm_number} ({req_sys_id})")

    att_params = {
        "sysparm_query": f"table_name=sc_req_item^table_sys_id={req_sys_id}",
        "sysparm_fields": "sys_id,file_name"
    }
    att_response = requests.get(attachments_url, headers=headers, params=att_params)
    if att_response.status_code != 200:
        print("âŒ Failed to get attachments:", att_response.text)
        continue

    attachments = att_response.json().get("result", [])

    for att in attachments:
        filename = att["file_name"]
        if not filename.lower().endswith(".pdf"):
            continue

        pdf_sys_id = att["sys_id"]
        download_url = download_url_template.format(sys_id=pdf_sys_id)
        print(f"â¬‡ï¸ Attempting to download and inspect: {filename}...")

        file_resp = requests.get(download_url, headers=headers)
        if file_resp.status_code != 200 or not file_resp.content:
            print(f"âŒ Failed to download {filename}. Status: {file_resp.status_code}")
            continue

        try:
            reader = PdfReader(io.BytesIO(file_resp.content))
            first_page_text = reader.pages[0].extract_text() or ""

            # âŒ Skip if your_condition on page 1

            if re.search(r"\byour_condition\b", first_page_text, re.IGNORECASE):
                print("âŒ Found 'Viya 3' or 'Viya 3.5' on first page. Skipping PDF.")
                viya3_skipped_count += 1
                continue

            # âœ… Check for "your condition" on any page

            viya_found = re.search(r"\byour_condition\b", first_page_text, re.IGNORECASE) is not None
            if not viya_found:
                for i, page in enumerate(reader.pages[1:], start=2):
                    text = page.extract_text() or ""
                    if re.search(r"\byour_condition\b", text, re.IGNORECASE):
                        print(f"âœ… Found 'Viya 4' on page {i}.")
                        viya_found = True
                        break
            if not viya_found:
                print("âš ï¸ 'Viya 4' not found in any pages. Skipping PDF.")
                viya4_not_found_count += 1
                continue

            # âœ… Combine all pages for full text search (only if Viya 4 found)

            full_text = first_page_text
            for page in reader.pages[1:]:
                text = page.extract_text()
                if text:
                    full_text += "\n" + text

            # RITM from text (if not in filename)

            ritm_match = re.search(r"(RITM\d+)", full_text, re.IGNORECASE)
            if ritm_match:
                ritm_from_text = ritm_match.group(1).upper()
                if not filename.upper().startswith(ritm_from_text):
                    final_filename = f"{ritm_from_text}_{filename}"
                else:
                    final_filename = filename
                print(f"ðŸ“Œ Found RITM in PDF: {ritm_from_text}")
            else:
                print("âš ï¸ No RITM found in content. Skipping PDF.")
                continue

            # --- Year logic: Detect ANY year, then check if it's the TARGET_YEAR ---

            found_year = None

            # Broad search for any 4-digit year starting with '20'
            year_patterns = [
                r"\b(20\d{2})\b", # Matches any 4-digit year starting with 20 (e.g., 2022, 2023, 2024, 2025)
                # Broader date formats that capture any 20XX year:
                r"\b\d{1,2}\s+[A-Za-z]+\s+(20\d{2})\b",
                r"\b[A-Za-z]+\s+\d{1,2},\s+(20\d{2})\b",
                r"\b(?:\d{1,2})[/-](?:\d{1,2})[/-](20\d{2})\b",
                r"\b(20\d{2})[/-](?:\d{1,2})[/-](?:\d{1,2})\b",
                r"\b(?:\d{1,2})[.](?:\d{1,2})[.](20\d{2})\b",
                r"\b(20\d{2})[.](?:\d{1,2})[.](?:\d{1,2})\b"
            ]

            for pattern in year_patterns:
                match = re.search(pattern, full_text) or re.search(pattern, filename)
                if match:
                    found_year = match.group(1)
                    print(f"ðŸ“… Extracted year from content/filename: {found_year}")
                    break

            # If no year was found, or it's not the TARGET_YEAR, skip it.

            if not found_year or found_year != TARGET_YEAR:
                if found_year: # A year was found, but it's not the target year
                    print(f"ðŸš« Skipping PDF: Found year '{found_year}', but only '{TARGET_YEAR}' files are processed.")
                else: # No year was found at all
                    print(f"ðŸš« Skipping PDF: Year not identified (found 'None'). Only '{TARGET_YEAR}' files are processed.")
                non_target_year_skipped_count += 1
                continue # Skip this PDF entirely

            print(f"ðŸ§  Year: {found_year}") # This will always be TARGET_YEAR if code reaches here

            # Save file
            # The year_folder is already based on the TARGET_YEAR at the script start.
            # No need to rejoin save_folder and found_year here, as save_folder already contains the year.
            file_path = os.path.join(save_folder, final_filename)

            # Duplicate check
            if is_duplicate(file_path, file_resp):
                print(f"â™»ï¸ Skipping duplicate file: {final_filename}")
                continue

            download_pdf(file_resp, file_path, final_filename)
            viya4_saved_count += 1

        except Exception as e:
            print(f"ðŸš« Failed to process {filename}: {e}")

# === Summary ===

print("\nðŸŽ‰ All done!")
print(f"\nðŸ“Š Summary for {TARGET_YEAR}:")
print(f"âœ… Total PDFs saved for {TARGET_YEAR}: {viya4_saved_count} file(s)")
print(f"ðŸš« Skipped (Not identified as {TARGET_YEAR}): {non_target_year_skipped_count}")
